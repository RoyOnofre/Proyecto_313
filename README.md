# Proyecto 313
practica final de sis313
üöÄ Proyecto Final SIS313: Sistema de Salas de Espera y Filas Virtuales (Virtual Queue)
Asignatura: SIS313: Infraestructura, Plataformas Tecnol√≥gicas y Redes
Semestre: 2/2025
Docente: Ing. Marcelo Quispe Ortega
üë• Miembros del Equipo (Grupo Virtual Queue)

Nombre Completo	Rol en el Proyecto	Contacto (GitHub/Email)
Duran Chambi Benjamin Ricardo	Arquitecto de Backend & Proxy: Encargado de la VM-PROXY y VM-APP (Nginx/Node.js). Dise√±o de la l√≥gica de encolamiento.	Ricardo
Escobar Moscoso Jorge Gabriel	Administrador de Datos: Encargado de la VM-REDIS. Gesti√≥n de persistencia en memoria y optimizaci√≥n de consultas.	https://github.com/jogaesmo
Onofre Alanoca Roy	Ingeniero de Observabilidad: Encargado de la VM-MON (Prometheus/Grafana). Monitoreo y auditor√≠a de m√©tricas.	https://github.com/RoyOnofre
üéØ I. Objetivo del Proyecto
Objetivo: Dise√±ar e implementar una arquitectura de Cola Virtual (Virtual Waiting Room) distribuida para el sistema universitario SUNiver, capaz de interceptar el 100% del tr√°fico entrante, aplicar Rate Limiting para limitar la concurrencia a un umbral seguro (ej. 100 usuarios/minuto) y redirigir el exceso de tr√°fico a una sala de espera est√°tica, garantizando as√≠ la disponibilidad del servicio cr√≠tico bajo condiciones de saturaci√≥n.
üí° II. Justificaci√≥n e Importancia
Justificaci√≥n: El problema recurrente durante las fechas de inscripci√≥n es la Denegaci√≥n de Servicio Involuntaria (saturaci√≥n de usuarios), lo que genera una Falla de Continuidad Operacional (T1). Este proyecto es vital porque:
1.	Transforma el Fallo en Espera: Convierte una ca√≠da del servidor (Error 503) en una espera ordenada y controlada.
2.	Protecci√≥n Centralizada (T5): Implementa una capa de protecci√≥n intermedia (Nginx) que desacopla la carga masiva del servidor de aplicaci√≥n, mitigando ataques de fuerza bruta y DDoS Capa 7 con Rate Limiting.
3.	Optimizaci√≥n Extrema (T4): Utiliza Redis (en RAM) para la gesti√≥n del estado de la cola, resolviendo la concurrencia a latencias de sub-milisegundos, algo inviable con bases de datos tradicionales.
üõ†Ô∏è III. Tecnolog√≠as y Conceptos Implementados
3.1. Tecnolog√≠as Clave
Tecnolog√≠a	Rol en el Proyecto (Componente)	Funci√≥n Espec√≠fica
Nginx (VM-PROXY)	Gateway y Reverse Proxy	Protege la topolog√≠a interna y aplica el m√≥dulo limit_req para filtrar peticiones abusivas (Rate Limiting).
Node.js (Express) (VM-APP)	Servidor de Aplicaci√≥n / L√≥gica	Ejecuta la l√≥gica de Portero: consulta el estado de la cola a Redis y decide si servir la p√°gina de Login o la Sala de Espera HTML.
Redis (VM-REDIS)	Gesti√≥n de Estado Global (Memoria)	Base de Datos NoSQL en memoria RAM. Mantiene el contador at√≥mico del aforo en tiempo real, garantizando la "Verdad √önica" para la l√≥gica de admisi√≥n.
Prometheus / Grafana (VM-MON)	Observabilidad y Auditor√≠a	Prometheus hace scraping de m√©tricas de la aplicaci√≥n, y Grafana visualiza la saturaci√≥n de tr√°fico y el comportamiento de la cola en Dashboards en tiempo real.
Tailscale / Avahi (mDNS)	Networking Transparente	Proporciona una interconexi√≥n de las VMs mediante nombres de dominio .local, facilitando la movilidad y el descubrimiento de servicios sin IPs est√°ticas.
3.2. Conceptos de la Asignatura Puestos en Pr√°ctica (T1 - T6)
Concepto	Implementaci√≥n en el Proyecto
Alta Disponibilidad (T2) y Tolerancia a Fallos:	‚úÖ Implementaci√≥n de un patr√≥n Circuit Breaker L√≥gico que previene la ca√≠da en cascada del servidor principal mediante el desv√≠o de tr√°fico a la sala de espera est√°tica.
Seguridad y Hardening (T5):	‚úÖ Uso de Rate Limiting en el Proxy Inverso (Nginx) para mitigar ataques de fuerza bruta y denegaci√≥n de servicio (DDoS) en Capa 7.
Automatizaci√≥n y Gesti√≥n (T6):	‚úÖ Configuraci√≥n de servicios systemd para el arranque autom√°tico y la recuperaci√≥n de servicios (Nginx, Redis, App) tras reinicios no programados.
Balanceo de Carga/Proxy (T3/T4):	‚úÖ Configuraci√≥n de Upstreams en Nginx para abstraer la ubicaci√≥n real de los servidores de aplicaci√≥n y permitir escalabilidad horizontal futura.
Monitoreo (T4/T1):	‚úÖ Implementaci√≥n de un stack de observabilidad completo (M√©tricas RED) para detectar cuellos de botella y auditar la estabilidad bajo picos de carga.
Networking Avanzado (T3):	‚úÖ Implementaci√≥n de resoluci√≥n de nombres interna (mDNS) y segmentaci√≥n de servicios en distintas VMs/Hosts, simulando un entorno de producci√≥n real.
üåê IV. Dise√±o de la Infraestructura y Topolog√≠a
4.1. Dise√±o Esquem√°tico
El dise√±o se basa en la segmentaci√≥n f√≠sica de los servicios en 4 VMs distribuidas en hosts distintos, comunicadas por nombre de dominio a trav√©s de la red LAN (mDNS).
VM/Host	Rol	IP L√≥gica / Hostname	Software Principal	Capa	SO
VM-PROXY	Gateway / Rate Limiter	proxy-server.local	Nginx	Acceso	Ubuntu 22.04
VM-APP	L√≥gica de Negocio / Worker	app-server.local	Node.js v20	Aplicaci√≥n	Ubuntu 22.04
VM-REDIS	Gesti√≥n de Estado (Cola)	redis-server.local	Redis Server 7	Datos	Ubuntu 22.04
VM-MON	Monitoreo y Alertas	monitor-server.local	Prometheus + Grafana	Gesti√≥n	Ubuntu 22.04
Dise√±o de la Infraestructura 







4.2. Estrategia Adoptada
‚óè	Estrategia de Desacoplamiento: Se separ√≥ el Proxy, la L√≥gica y el Estado en m√°quinas virtuales distintas. Esto garantiza que la VM-PROXY pueda seguir respondiendo con p√°ginas de Sala de Espera, incluso si la VM-APP se sobrecarga o falla, manteniendo el control del acceso.
‚óè	Estrategia de Optimizaci√≥n (Redis): Se eligi√≥ Redis sobre una base de datos SQL porque las operaciones de incremento/decremento de cola deben ser at√≥micas y de latencia cero. El uso de la RAM garantiza que el contador de cupos nunca sea el cuello de botella (T4).
‚óè	Networking H√≠brido: La configuraci√≥n del descubrimiento de servicios mediante mDNS (.local) permite que las m√°quinas se encuentren din√°micamente sin depender de IPs est√°ticas fijas, facilitando la movilidad del despliegue.
üìã V. Gu√≠a de Implementaci√≥n y Puesta en Marcha

 Parte 0. Pre-Requisito General (Hacer en TODAS las VMs)
Prop√≥sito: Establecer la conectividad por nombre de host (hostname), crucial para que los servicios se encuentren entre s√≠ sin depender de IPs est√°ticas.
M√°quinas Afectadas: VM-REDIS, VM-APP, VM-PROXY, VM-MON.
1.	Actualizar e Instalar herramientas b√°sicas y Avahi (mDNS):
Instala herramientas de red y el servicio Avahi para el descubrimiento de servicios por nombre (.local).
sudo apt update
sudo apt install -y net-tools curl avahi-daemon libnss-mdns


2.	Prueba de Conectividad:
Desde cualquier VM, verifica la conexi√≥n con otra usando su nombre l√≥gico.
Ejemplo: ping app-server.local
Si la prueba es exitosa, el networking est√° listo.
Parte 1. VM-REDIS (La Memoria - Capa de Datos)
Hostname: redis-server.local
Funci√≥n: Almacenar el contador at√≥mico de usuarios activos.
Sigue estos pasos para la configuraci√≥n:
1.	Instalar el servicio Redis Server (Se instala la base de datos ultrarr√°pida en RAM):
sudo apt install redis-server -y


2.	Editar la configuraci√≥n para permitir conexiones remotas (El servidor de la l√≥gica VM-APP debe poder conectarse):
sudo nano /etc/redis/redis.conf


3.	CAMBIO CLAVE: Permitir conexiones externas. Dentro del archivo, encuentra la l√≠nea bind 127.0.0.1 y c√°mbiala a:
bind 0.0.0.0

# Abrir el archivo de configuraci√≥n
sudo nano /etc/redis/redis.conf 
# Cambiar 'bind 127.0.0.1' por:
# bind 0.0.0.0 
# Guardar y reiniciar
sudo systemctl restart redis-server

4.	Reiniciar el servicio para aplicar cambios:
sudo systemctl restart redis-server


5.	Limpieza (OPCIONAL, pero recomendado para demo):
redis-cli FLUSHALL


Parte 2. VM-APP (El Cerebro - Capa de Aplicaci√≥n)
Hostname: app-server.local
Funci√≥n: Ejecutar la l√≥gica de admisi√≥n (comparar cupo con Redis) y servir la aplicaci√≥n en el puerto 3000.
Sigue estos pasos para la configuraci√≥n:
1.	Instalar Node.js (Se instala el entorno de ejecuci√≥n para la aplicaci√≥n de l√≥gica):
sudo apt install nodejs -y


2.	Instalar las dependencias de la aplicaci√≥n (Necesitas Express para el servidor web y Redis para la conexi√≥n a la VM-REDIS):
npm install express redis


3.	Crear la l√≥gica del portero (Introduce el c√≥digo de decisi√≥n en un nuevo archivo):
nano index.js
// Archivo index.js
const express = require('express');
const redis = require('redis');
const app = express();
const PORT = 3000;
const MAX_CAPACITY = 5; // L√≠mite de usuarios concurrentes permitidos
const REDIS_KEY = 'suniver:active_users';
// 1. CONEXI√ìN AL HOST REMOTO (VM-REDIS)
const client = redis.createClient({
    socket: {
        host: 'redis-server.local', // Usamos el hostname de la VM-REDIS
        port: 6379
    }
});
client.connect();
client.on('error', err => console.error('Redis Client Error:', err));

// 2. LOGICA DEL SERVIDOR
app.get('/', async (req, res) => {
    try {
        // Incrementamos el contador: preguntamos a Redis cuantos hay Y sumamos 1
        const activeUsers = await client.incr(REDIS_KEY);

        if (activeUsers <= MAX_CAPACITY) {
            // ESCENARIO A: HAY CUPO (ENTRA - Pantalla VERDE)
            // Simular que el usuario se queda 10 segundos y luego se va (decr)
            setTimeout(() => client.decr(REDIS_KEY), 10000); 
            res.send(`
                <body style="background-color: #2ecc71; color: white; text-align: center; padding-top: 50px; font-family: sans-serif;">
                    <h1>‚úÖ BIENVENIDO AL SISTEMA (Login)</h1>
                    <p>Usuarios Activos: ${activeUsers} / ${MAX_CAPACITY}</p>
                    <p>Su sesi√≥n termina en 10 segundos.</p>
                </body>
            `);
        } else {
            // ESCENARIO B: NO HAY CUPO (SALA DE ESPERA - Pantalla NARANJA)
            // Devolvemos la cuenta ya que no se le permiti√≥ pasar
            await client.decr(REDIS_KEY); 
            res.send(`
                <body style="background-color: #e67e22; color: white; text-align: center; padding-top: 50px; font-family: sans-serif;">
                    <h1>üü† SALA DE ESPERA - Capacidad Maxima Alcanzada</h1>
                    <p>El sistema esta saturado. Recargando en 5 segundos...</p>
                </body>
                <meta http-equiv="refresh" content="5"> 
            `);
        }
    } catch (error) {
        // En caso de fallo de Redis o l√≥gica
        console.error(error);
        res.status(503).send('<h1>üî¥ ERROR 503: Servidor temporalmente no disponible.</h1>');
    }
});
// 3. INICIO
app.listen(PORT, () => {
    console.log(`App corriendo en puerto ${PORT}`);
});



4.	Ejecutar el servidor de la aplicaci√≥n (Inicia la aplicaci√≥n en el puerto 3000 en segundo plano):
nohup node index.js &


Parte 3. VM-PROXY (El Portero - Capa de Acceso)
Hostname: proxy-server.local
Funci√≥n: Punto de entrada √∫nico. Aplica Rate Limiting (T5) y enruta el tr√°fico al servidor de aplicaci√≥n (Proxy Inverso T3/T4).
Sigue estos pasos para la configuraci√≥n:
1.	Instalar el servidor Nginx (Servidor proxy de alto rendimiento):
sudo apt install nginx -y


2.	Configurar Rate Limiting (T5) (Edici√≥n del archivo de configuraci√≥n principal de Nginx):
sudo nano /etc/nginx/nginx.conf


3.	CAMBIO CLAVE: Definir la regla de Rate Limit. Debajo de la l√≠nea http {, a√±ade la siguiente directiva (10 peticiones por segundo por IP de usuario):
limit_req_zone $binary_remote_addr zone=one:10m rate=10r/s;


4.	Configurar Proxy Inverso (T3/T4) (Apuntar al servidor de la l√≥gica VM-APP):
sudo nano /etc/nginx/sites-available/default
 
              Clave: Definir el upstream hacia la VM-APP.
# Bloque de configuracion de Nginx (dentro de sites-available/default)

# 1. Definicion del UPSTREAM (el destino real)
upstream backend_app {
    # Apunta a la VM-APP en su puerto interno 3000
    server app-server.local:3000; 
}

server {
    listen 80 default_server;
    listen [::]:80 default_server;

    server_name _;

    location / {
        # 2. Aplicar el filtro de Rate Limiting
        limit_req zone=one burst=5 nodelay; 
        
        # 3. Proxy Pass (Enviar al Cerebro)
        proxy_pass http://backend_app;

        # Opcional: Agregar cabeceras para que el servidor de la app sepa la IP real del usuario
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}

5.	CAMBIO CLAVE: Definir Upstream y aplicar Rate Limit. Dentro del bloque server {, reemplaza la secci√≥n location / por el c√≥digo que define el upstream a app-server.local:3000 y aplica la regla de Rate Limit. (Ver el c√≥digo Nginx de location / en tu entorno).
6.	Probar la sintaxis y reiniciar Nginx (Asegura que la configuraci√≥n no tenga errores):
sudo nginx -t && sudo systemctl restart nginx

Parte 4. VM-MON (Los Ojos - Capa de Gesti√≥n)
Hostname: monitor-server.local
Funci√≥n: Visualizar el estado de la cola y las m√©tricas de carga en tiempo real (Auditor√≠a T4).
Sigue estos pasos para la configuraci√≥n:
1.	Instalar Prometheus y Grafana (El stack de observabilidad):
(Se asume que los paquetes est√°n instalados)
2.	Configurar el Scraper de Prometheus (Indicar a Prometheus qu√© m√©tricas recolectar):
sudo nano /etc/prometheus/prometheus.yml

# ... dentro de 'scrape_configs'
- job_name: 'node_app'
  scrape_interval: 5s
  static_configs:
    - targets: ['app-server.local:3000'] # Apunta a la VM-APP

3.	CAMBIO CLAVE: A√±adir el objetivo de la VM-APP. Dentro de scrape_configs:, a√±ade la configuraci√≥n para recolectar datos de la ruta /metrics de la VM-APP cada 5 segundos. (Ver el c√≥digo Prometheus en tu entorno).
4.	Reiniciar Prometheus (Inicia la recolecci√≥n de datos desde la VM-APP):
sudo systemctl restart prometheus


5.	Configurar Grafana: Abrir Grafana en el navegador (puerto 3000 o 80). Agregar Prometheus como Data Source y crear/importar un dashboard simple.

5.1. Ficheros de Configuraci√≥n Clave
Fichero	Ubicaci√≥n / Rol	Funci√≥n Cr√≠tica
index.js	VM-APP (L√≥gica)	Algoritmo de decisi√≥n (If cupo -> Login, Else -> SalaEspera) y conexi√≥n remota a redis-server.local.
nginx.conf	VM-PROXY (Proxy)	Define el upstream a app-server.local y la directiva Rate Limiting para el control de acceso.
redis.conf	VM-REDIS (Datos)	Establece bind 0.0.0.0 para permitir la comunicaci√≥n remota con la VM-APP.
prometheus.yml	VM-MON (Monitoreo)	Configura el scraping (recolecci√≥n) de m√©tricas de la VM-APP.
‚ö†Ô∏è VI. Pruebas y Validaci√≥n
Prueba Realizada	Resultado Esperado	Resultado Obtenido
Test de Estr√©s (Saturaci√≥n)	Al superar el l√≠mite de N usuarios, el usuario N+1 debe ver la pantalla naranja de "Sala de Espera", no el Error 503.	OK: El sistema redirigi√≥ correctamente al usuario 6 a la cola.
Test de Recuperaci√≥n Autom√°tica	Cuando un usuario activo abandona el sistema (timeout), la cola debe avanzar autom√°ticamente.	OK: El usuario en espera ingres√≥ autom√°ticamente en el siguiente refresco.
Prueba de Monitoreo en Vivo	Grafana debe mostrar el pico de tr√°fico y el estancamiento de usuarios activos en el l√≠mite m√°ximo.	OK: Dashboard reflej√≥ la curva de saturaci√≥n en tiempo real, validando la estabilidad (T1).
Validaci√≥n de Conectividad	Las VMs deben comunicarse por hostname (.local) independientemente de la IP asignada por el DHCP.	OK: Pings y conexiones de base de datos exitosas por nombre.
Objetivo a llegar.
 
üìö VII. Conclusiones y Lecciones Aprendidas
Logro Principal: Demostramos que la escalabilidad y la disponibilidad se logran mediante una arquitectura de software inteligente (Desacoplamiento), no solo con la adici√≥n de m√°s hardware. El sistema transform√≥ un escenario de fallo (Error 500) en una experiencia de usuario controlada (Sala de Espera).
Lecci√≥n Aprendida: La importancia de Redis (T4) es cr√≠tica. Intentar gestionar el estado de la cola en una base de datos tradicional hubiera introducido latencia, convirtiendo al contador en el principal cuello de botella. La gesti√≥n en RAM fue esencial.
Mejora Futura: Para un despliegue de producci√≥n real, implementar√≠amos un cl√∫ster de Redis (Sentinel) para evitar que la VM-REDIS sea un punto √∫nico de fallo, y asegurar√≠amos la capa de acceso con HTTPS 
